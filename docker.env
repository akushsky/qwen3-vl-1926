# LLM Configuration
LLM_ENDPOINT=http://llm-service:8000/v1/chat/completions
LLM_MODEL=Qwen/Qwen3-VL-8B-Instruct-FP8
OPENAI_API_KEY=EMPTY

# Web App Configuration
FLASK_ENV=production
FLASK_APP=app.py

# GPU Configuration (for nvidia-docker)
CUDA_VISIBLE_DEVICES=0
